# Project: Data Modeling with Apache Cassandra

## Datasets
This project uses the **"event_data"** dataset, which consists of CSV files partitioned by date. Each file contains event records, and examples of file paths include:

- `event_data/2018-11-08-events.csv`
- `event_data/2018-11-09-events.csv`

## Project Overview
A Jupyter Notebook file serves as the project template, providing a structured workflow for processing and modeling the dataset. The **"event_datafile_new.csv"** file is generated by consolidating multiple CSV files into a single dataset. The data is then modeled into Apache Cassandra tables based on the queries that need to be executed.

The template includes predefined queries, guiding the table design to ensure efficient data retrieval. Data is loaded into the Cassandra tables, followed by query execution to validate the database structure and ETL pipeline.

## NoSQL Database Modeling (Apache Cassandra)
The data model is designed to support specific queries by structuring tables accordingly. Keyspace creation and table definitions are formulated to ensure proper partitioning and clustering of data. Each table includes **CREATE** statements tailored to meet query requirements, and **IF NOT EXISTS** clauses are used to prevent duplicate table creation. **DROP TABLE** statements are included to facilitate easy recreation and testing.

After the tables are created, data is inserted using **INSERT** statements, followed by **SELECT** queries to verify the correctness of the database schema and the stored data.

## ETL Pipeline Development
The ETL pipeline processes event files from the **"event_data"** directory, extracting relevant information and consolidating it into a structured format. Python scripts iterate through the dataset, process records, and store them in a new CSV file. The processed data is then loaded into Apache Cassandra tables, ensuring that the database structure aligns with query requirements.

Once the data is stored, **SELECT** statements are executed to confirm that records have been properly ingested and that queries return the expected results. This process ensures that the NoSQL database effectively supports the required analytical queries.
